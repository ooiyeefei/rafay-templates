# These values are structured for the otwld/ollama Helm chart version 0.29.0
# and your specific EKS cluster configuration.

# --- Scheduling Configuration ---
%{ if ollama_on_gpu ~}
nodeSelector:
  accelerator: "nvidia"

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
%{ endif ~}


# --- Persistence Configuration ---
persistentVolume:
  enabled: true
  # OMITTED: storageClass is removed to use the cluster's default.
  size: 50Gi


# --- Application Configuration ---
ollama:
  # This enables GPU features if the flag is true.
  gpu:
    enabled: ${ollama_on_gpu}
    type: "nvidia"
    number: 1

# These values are structured for the otwld/ollama Helm chart version 0.29.0
# and your specific EKS cluster configuration.

# --- Scheduling Configuration ---
%{ if ollama_on_gpu ~}
nodeSelector:
  accelerator: "nvidia"

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
%{ endif ~}


# --- Persistence Configuration ---
persistentVolume:
  enabled: true
  # OMITTED: storageClass is removed to use the cluster's default.
  size: 50Gi


# --- Application Configuration ---
ollama:
  # This enables GPU features if the flag is true.
  gpu:
    enabled: ${ollama_on_gpu}
    type: "nvidia"
    number: 1

initContainers:
- name: "pull-models"
  image: "ollama/ollama:0.1.38" # Use the same image version
  # Mount the persistent volume to the same path
  volumeMounts:
    - name: ollama-data
      mountPath: /root/.ollama
  # The command to run
  command:
    - "/bin/sh"
    - "-c"
    - |
      echo "Init Container: Starting model pulls..."
      %{ for model in ollama_models ~}
      ollama pull ${model}
      %{ endfor ~}
      echo "Init Container: All model pulls complete."